{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXijkfD2Jj2q"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUVKLDwiIokM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1768406709913,
     "user": {
      "displayName": "Kalana S.",
      "userId": "14466272889358370869"
     },
     "user_tz": -330
    },
    "id": "LSCWXJMrIsAb",
    "outputId": "2b9bb167-f9e4-4f40-c25f-5ba91dc7fa72"
   },
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/allahhitler/ocr-synthetic-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aia_R1vbIxLz"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/content/ocr-synthetic-dataset\"\n",
    "IMG_DIR = os.path.join(DATA_ROOT, \"images\")\n",
    "LABEL_FILE = os.path.join(DATA_ROOT, \"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1768406712498,
     "user": {
      "displayName": "Kalana S.",
      "userId": "14466272889358370869"
     },
     "user_tz": -330
    },
    "id": "rF6OVENdJ71k",
    "outputId": "4425f40a-097a-45e3-9b68-6c2d0e018eb5"
   },
   "outputs": [],
   "source": [
    "print(\"Images:\", len(os.listdir(IMG_DIR)))\n",
    "\n",
    "with open(LABEL_FILE) as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sidnaYFLKDJk"
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 32\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIMeu8CbKKpE"
   },
   "outputs": [],
   "source": [
    "# Vocabulary (CTC-SAFE)\n",
    "\n",
    "CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n",
    "\n",
    "char_to_num = tf.keras.layers.StringLookup(\n",
    "    vocabulary=CHARS,\n",
    "    mask_token=None,\n",
    "    oov_token=None\n",
    ")\n",
    "\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary()[1:],\n",
    "    invert=True\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(char_to_num.get_vocabulary()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utz9R6WKKRjl"
   },
   "outputs": [],
   "source": [
    "# Dataset Loader (CRITICAL PART)\n",
    "\n",
    "def parse_labels(label_file):\n",
    "    samples = []\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            img, text = line.strip().split()\n",
    "            samples.append((os.path.join(IMG_DIR, img), text))\n",
    "    return samples\n",
    "\n",
    "samples = parse_labels(LABEL_FILE)\n",
    "np.random.shuffle(samples)\n",
    "\n",
    "split = int(0.9 * len(samples))\n",
    "train_samples = samples[:split]\n",
    "val_samples = samples[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG66iwsUKVsl"
   },
   "outputs": [],
   "source": [
    "# Image & Label Processing\n",
    "\n",
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=1)\n",
    "    h, w = tf.shape(img)[0], tf.shape(img)[1]\n",
    "    new_w = tf.cast(w * IMG_HEIGHT / h, tf.int32)\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, new_w))\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "def encode_label(text):\n",
    "    chars = tf.strings.unicode_split(text, \"UTF-8\")\n",
    "    return char_to_num(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeOJvTMYKYzO"
   },
   "outputs": [],
   "source": [
    "# tf.data Pipeline (PADDED WIDTH)\n",
    "\n",
    "def make_dataset(samples, training=True):\n",
    "    paths = [s[0] for s in samples]\n",
    "    labels = [s[1] for s in samples]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    def process(p, l):\n",
    "        return {\n",
    "            \"image\": load_image(p),\n",
    "            \"label\": encode_label(l)\n",
    "        }\n",
    "\n",
    "    ds = ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if training:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    ds = ds.padded_batch(\n",
    "        BATCH_SIZE,\n",
    "        padded_shapes={\n",
    "            \"image\": [IMG_HEIGHT, None, 1],\n",
    "            \"label\": [None]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(train_samples, True)\n",
    "val_ds = make_dataset(val_samples, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFPbUONaKbi-"
   },
   "outputs": [],
   "source": [
    "# CRNN Model\n",
    "\n",
    "def build_crnn():\n",
    "    inputs = tf.keras.Input(shape=(IMG_HEIGHT, None, 1))\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Permute((2, 1, 3))(x)\n",
    "    x = tf.keras.layers.Reshape((-1, x.shape[2] * x.shape[3]))(x)\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "    )(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        NUM_CLASSES,\n",
    "        activation=\"softmax\"\n",
    "    )(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"crnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ySlBL4tKeYn"
   },
   "outputs": [],
   "source": [
    "# Correct Loss Handling\n",
    "\n",
    "class CTCModel(tf.keras.Model):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.backbone(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"label\"]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.backbone(images, training=True)\n",
    "\n",
    "            batch_size = tf.shape(preds)[0]\n",
    "            time_steps = tf.shape(preds)[1]\n",
    "\n",
    "            input_len = tf.fill([batch_size, 1], time_steps)\n",
    "\n",
    "            label_len = tf.math.count_nonzero(labels, axis=1, keepdims=True)\n",
    "\n",
    "            loss = tf.keras.backend.ctc_batch_cost(\n",
    "                labels, preds, input_len, label_len\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": tf.reduce_mean(loss)}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"label\"]\n",
    "\n",
    "        preds = self.backbone(images, training=False)\n",
    "\n",
    "        batch_size = tf.shape(preds)[0]\n",
    "        time_steps = tf.shape(preds)[1]\n",
    "\n",
    "        input_len = tf.fill([batch_size, 1], time_steps)\n",
    "        label_len = tf.math.count_nonzero(labels, axis=1, keepdims=True)\n",
    "\n",
    "        loss = tf.keras.backend.ctc_batch_cost(\n",
    "            labels, preds, input_len, label_len\n",
    "        )\n",
    "\n",
    "        return {\"loss\": tf.reduce_mean(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1722418,
     "status": "ok",
     "timestamp": 1768411300538,
     "user": {
      "displayName": "Kalana S.",
      "userId": "14466272889358370869"
     },
     "user_tz": -330
    },
    "id": "XUIZC6IDKhqv",
    "outputId": "2800c17e-4c1d-4731-dade-deab46fdc49f"
   },
   "outputs": [],
   "source": [
    "# Compile and Train\n",
    "\n",
    "backbone = build_crnn()\n",
    "model = CTCModel(backbone)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDOznK8xKhg_"
   },
   "outputs": [],
   "source": [
    "# Decoding (GREEDY)\n",
    "\n",
    "def decode(pred):\n",
    "    batch_size = tf.shape(pred)[0]\n",
    "    time_steps = tf.shape(pred)[1]\n",
    "\n",
    "    input_len = tf.fill([batch_size], time_steps)\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(pred, input_len, greedy=True)\n",
    "\n",
    "    texts = []\n",
    "    for seq in decoded[0]:\n",
    "        seq = tf.boolean_mask(seq, seq != -1)\n",
    "        text = tf.strings.reduce_join(num_to_char(seq)).numpy().decode()\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1768411318155,
     "user": {
      "displayName": "Kalana S.",
      "userId": "14466272889358370869"
     },
     "user_tz": -330
    },
    "id": "LVNVwwz2Kltf",
    "outputId": "f2bfdffc-1cb6-4dae-a830-c295309aca29"
   },
   "outputs": [],
   "source": [
    "for batch in val_ds.take(1):\n",
    "    preds = backbone(batch[\"image\"], training=False)\n",
    "    texts = decode(preds)\n",
    "\n",
    "    for i in range(5):\n",
    "        gt = tf.strings.reduce_join(\n",
    "            num_to_char(batch[\"label\"][i])\n",
    "        ).numpy().decode()\n",
    "        print(\"GT:\", gt, \"| PRED:\", texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22S-beFUJErs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tGOOKEiKnOP"
   },
   "outputs": [],
   "source": [
    "backbone.save(\"/content/drive/MyDrive/Machine_Learning/Image_Based/Word_Recognition/synth90k_crnn.keras\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+N9BcsjTZ2MFKqAMvNuo1",
   "gpuType": "T4",
   "mount_file_id": "1GW-GSfgcEdU8sUxVduztrzkZH72Ogcxn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
